---
title: "predict_citibike.Rmd"
output: html_document
date: "2023-06-19"
---

```{r setup, include=FALSE}

library(scales)
library(tidyverse)
library(modelr)

theme_set(theme_bw())

knitr::opts_chunk$set(echo = TRUE)
```

## Set up 
use the trips_per_day.tsv file
-> one row for each day, the number of trips taken on that day, and the min temperature on that day


```{r load data}

trips_per_day <- read_tsv('trips_per_day.tsv')
```
## Data set up 
split the data into training set and test set(9:1), then shuffle the data in the training set 


```{r}
#use 90 percent of dataset as training set and 10 percent for testing set, stores into sample_trips
set.seed(42)

sample_trips <- sample(c(TRUE,FALSE), nrow(trips_per_day), replace = TRUE, prob = c(0.9, 0.1))

train_dataset <- trips_per_day[sample_trips,]
# dimension of train_dataset: 321 obs of 8 variables

# Left out the 10% dataset for testing after model completed
test_dataset <- trips_per_day[!sample_trips,]
# dimension of test_dataset: 44 obs. of 8 variables


# Shuffle the data to 80% train and 20% validation split for the 90% of the sampling data. 
num_days <- nrow(train_dataset)
frac_train <- as.numeric(0.8)
num_train <- floor(num_days * frac_train)

# randomly sample rows for the training set 
ndx <- sample(1:num_days, num_train, replace=F)

# used to fit the model
trips_per_day_train <- train_dataset[ndx, ]

# used to evaluate the fit
trips_per_day_validate <- train_dataset[-ndx, ]

```
## Model 1- tmin 
fit a model and evaluate the model on the training and validateion datasets
adapted from [chapter 4 modeling basics](https://github.com/XiuwenZ/coursework/blob/master/week3/complexity_control.ipynb)

```{r}
K <- 1:8
train_err <- c()
validate_err <- c()
for (k in K) {
  
    # fit on the training data
    model <- lm(num_trips ~ poly(tmin, k, raw = T), data=trips_per_day_train)
    
    # evaluate on the training data
    train_err[k] <- sqrt(mean((predict(model, trips_per_day_train) - trips_per_day_train$num_trips)^2))

    # evaluate on the validate data
    validate_err[k] <- sqrt(mean((predict(model, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
}

plot_data <- data.frame(K, train_err, validate_err) %>%
  gather("split", "error", -K)

ggplot(plot_data, aes(x=K, y=error, color=split)) +
  geom_line() +
  scale_x_continuous(breaks=K) +
  xlab('Polynomial Degree') +
  ylab('RMSE')





# k-fold cross validation on the model 
set.seed(42)
num_folds <- 5
num_days <- nrow(train_dataset)

ndx <- sample(1:num_days, num_train, replace=F)

train_dataset <- train_dataset[ndx, ] %>%
  mutate(fold = (row_number() %% num_folds) + 1)

# fit a model for each polynomial degree
      # Note: loop through each polynomial degree, as before, but add an inner loop over folds to compute the average validation error
degrees <- 1:8
avg_validate_err <- c()
se_validate_err <- c()
for (d in degrees) {

  # do 5-fold cross-validation within each value of k
  validate_err <- c()
  for (f in 1:num_folds) {
    # fit on the training data
    trips_per_day_train <- filter(train_dataset, fold != f)
    model <- lm(num_trips ~ poly(tmin, d, raw = T), data=trips_per_day_train)

    # evaluate on the validation data
    trips_per_day_validate <- filter(train_dataset, fold == f)
    validate_err[f] <- sqrt(mean((predict(model, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
  }

  # compute the average validation error across folds
  # and the standard error on this estimate
  avg_validate_err[d] <- mean(validate_err)
  se_validate_err[d] <- sd(validate_err) / sqrt(num_folds)
}


# refit the model with the optimal number poly degree, 7 in this case
model <- lm(num_trips ~ poly(tmin, 7, raw = T), data = trips_per_day_train)

trips_per_day_train <- trips_per_day_train %>%
  add_predictions(model) %>%
  mutate(split = "train")
trips_per_day_validate <- trips_per_day_validate %>%
  add_predictions(model) %>%
  mutate(split = "validate")
plot_data <- bind_rows(trips_per_day_train, trips_per_day_validate)

ggplot(plot_data, aes(x = tmin, y = num_trips)) +
  geom_point(aes(color = split)) +
  geom_line(aes(y = pred)) +
  xlab('Minimum temperature') +
  ylab('Daily trips') +
  scale_y_continuous()



# plot the validate error, highlighting the value of k with the lowest average error
plot_data <- data.frame(degrees, avg_validate_err, se_validate_err)
ggplot(plot_data, aes(x=degrees, y=avg_validate_err)) +
  geom_pointrange(aes(ymin=avg_validate_err - se_validate_err,
                      ymax=avg_validate_err + se_validate_err,
                      color=avg_validate_err == min(avg_validate_err))) +
  geom_line(color = "red") +
  scale_x_continuous(breaks=1:12) +
  theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')

# # get the RMSE value of this model
# sqrt(mean((predict(model, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
# # output: 5466.696


model <- lm(num_trips ~ poly(tmin, 5, raw = T), data=trips_per_day_train)
sqrt(mean((predict(model, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
# [1] 5091.223


```
## model 2 - see the impact of precipitation on the number of rides
```{r}
set.seed(42)
degrees <- 1:8
train_err <- c()
validate_err <- c()
for (d in degrees) {
  
    # fit on the training data
    model2 <- lm(num_trips ~ poly(tmin, d, raw = T) + I(prcp>0), data=trips_per_day_train)
    
    # evaluate on the training data
    train_err[d] <- sqrt(mean((predict(model2, trips_per_day_train) - trips_per_day_train$num_trips)^2))

    # evaluate on the validate data
    validate_err[d] <- sqrt(mean((predict(model2, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
}

plot_data <- data.frame(degrees, train_err, validate_err) %>%
  gather("split", "error", -degrees)

ggplot(plot_data, aes(x=degrees, y=error, color=split)) +
  geom_line() +
  scale_x_continuous(breaks=degrees) +
  xlab('Polynomial Degree') +
  ylab('RMSE')

# fit a model for each polynomial degree
      # Note: loop through each polynomial degree, as before, but add an inner loop over folds to compute the average validation error
for (d in degrees) {

  # do 5-fold cross-validation within each value of k
  validate_err <- c()
  for (f in 1:num_folds) {
    # fit on the training data
    trips_per_day_train <- filter(train_dataset, fold != f)
    model <- lm(num_trips ~ poly(tmin, d, raw = T), data=trips_per_day_train)

    # evaluate on the validation data
    trips_per_day_validate <- filter(train_dataset, fold == f)
    validate_err[f] <- sqrt(mean((predict(model2, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
  }

  # compute the average validation error across folds
  # and the standard error on this estimate
  avg_validate_err[d] <- mean(validate_err)
  se_validate_err[d] <- sd(validate_err) / sqrt(num_folds)
}

# 
# # plot the validate error, highlighting the value of k with the lowest average error
plot_data <- data.frame(degrees, avg_validate_err, se_validate_err)
ggplot(plot_data, aes(x=degrees, y=avg_validate_err)) +
  geom_pointrange(aes(ymin=avg_validate_err - se_validate_err,
                      ymax=avg_validate_err + se_validate_err,
                      color=avg_validate_err == min(avg_validate_err))) +
  geom_line(color = "red") +
  scale_x_continuous(breaks=1:12)  +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')

# get the RMSE value of this model
# how to get the best degree? plot the degrees of the model 
model2 <- lm(num_trips ~ poly(tmin, k, raw = T) + I(prcp>0), data=trips_per_day_train)
sqrt(mean((predict(model2, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
# [1] 5057.61



```
#model3, snow or precp
```{r}
K <- 1:8
train_err <- c()
validate_err <- c()
for (k in K) {
  
    # fit on the training data
    model3 <- lm(num_trips ~ poly(tmin, k, raw = T) + I(prcp>0)+ I(snowd >2), data=trips_per_day_train)
    
    # evaluate on the training data
    train_err[k] <- sqrt(mean((predict(model, trips_per_day_train) - trips_per_day_train$num_trips)^2))

    # evaluate on the validate data
    validate_err[k] <- sqrt(mean((predict(model, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
}

plot_data <- data.frame(K, train_err, validate_err) %>%
  gather("split", "error", -K)

ggplot(plot_data, aes(x=K, y=error, color=split)) +
  geom_line() +
  scale_x_continuous(breaks=K) +
  xlab('Polynomial Degree') +
  ylab('RMSE')


#plot the validation error

# plot the validate error, highlighting the value of k with the lowest average error
plot_data <- data.frame(K, avg_validate_err, se_validate_err)
ggplot(plot_data, aes(x=K, y=avg_validate_err)) +
  geom_pointrange(aes(ymin=avg_validate_err - se_validate_err,
                      ymax=avg_validate_err + se_validate_err,
                      color=avg_validate_err == min(avg_validate_err))) +
  geom_line(color = "red") +
  scale_x_continuous(breaks=1:12) +
  theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')

# get the RMSE value of this model

model3 <- lm(num_trips ~ poly(tmin, 7, raw = T) + I(prcp>0)+ I(snow >0), data=trips_per_day_train)
sqrt(mean((predict(model3, trips_per_day_validate) - trips_per_day_validate$num_trips)^2))
# [1] 4562.426???
# 5013.71


save(model3, file = "best_model_prcp_n_snow.RData")
```


Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
